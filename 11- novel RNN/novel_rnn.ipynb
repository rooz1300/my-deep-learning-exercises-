{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Movie Review Sentiment Analysis with SimpleRNN\n",
    "\n",
    "This Jupyter notebook demonstrates a simple implementation of a Recurrent Neural Network (RNN) for sentiment analysis on the IMDB movie review dataset. The model uses the SimpleRNN layer from Keras to process sequential data.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Imports](#imports)\n",
    "2. [Data Loading and Preprocessing](#data-loading-and-preprocessing)\n",
    "3. [Model Building](#model-building)\n",
    "4. [Model Training and Evaluation](#model-training-and-evaluation)\n",
    "\n",
    "## Imports\n",
    "\n",
    "The necessary libraries and modules are imported at the beginning of the script.\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import keras\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "```\n",
    "\n",
    "## Data Loading and Preprocessing\n",
    "\n",
    "The IMDB dataset is loaded using `imdb.load_data()`. The dataset is split into training and testing sets. The sequences are then padded to a maximum length of 80 words using `pad_sequences()`.\n",
    "\n",
    "```python\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test =  pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "```\n",
    "\n",
    "## Model Building\n",
    "\n",
    "A simple sequential model is built using the `Sequential()` class from Keras. The model consists of an Embedding layer, a SimpleRNN layer, and a Dense layer with a sigmoid activation function.\n",
    "\n",
    "```python\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(SimpleRNN(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "The model is then compiled with the binary cross-entropy loss function, the Adam optimizer, and accuracy as the metric.\n",
    "\n",
    "```python\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "## Model Training and Evaluation\n",
    "\n",
    "The model is trained on the training data for 15 epochs using a batch size of 32. The testing data is used for validation during training.\n",
    "\n",
    "```python\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "```\n",
    "\n",
    "After training, the model is evaluated on the testing data, and the test loss and accuracy are printed.\n",
    "\n",
    "```python\n",
    "loss, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', loss)\n",
    "print('Test accuracy:', acc)\n",
    "```\n",
    "\n",
    "This notebook provides a basic example of using a SimpleRNN for sentiment analysis on the IMDB movie review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Epoch 1/15\n",
      "782/782 [==============================] - 79s 95ms/step - loss: 0.5599 - accuracy: 0.6968 - val_loss: 0.5756 - val_accuracy: 0.7032\n",
      "Epoch 2/15\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.4043 - accuracy: 0.8241 - val_loss: 0.4773 - val_accuracy: 0.7775\n",
      "Epoch 3/15\n",
      "782/782 [==============================] - 83s 106ms/step - loss: 0.3576 - accuracy: 0.8460 - val_loss: 0.4738 - val_accuracy: 0.7918\n",
      "Epoch 4/15\n",
      "782/782 [==============================] - 87s 111ms/step - loss: 0.3536 - accuracy: 0.8483 - val_loss: 0.5500 - val_accuracy: 0.7245\n",
      "Epoch 5/15\n",
      "782/782 [==============================] - 91s 117ms/step - loss: 0.3067 - accuracy: 0.8773 - val_loss: 0.5477 - val_accuracy: 0.7791\n",
      "Epoch 6/15\n",
      "782/782 [==============================] - 85s 109ms/step - loss: 0.3071 - accuracy: 0.8708 - val_loss: 0.5216 - val_accuracy: 0.7743\n",
      "Epoch 7/15\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.2781 - accuracy: 0.8898 - val_loss: 0.5918 - val_accuracy: 0.6886\n",
      "Epoch 8/15\n",
      "782/782 [==============================] - 66s 84ms/step - loss: 0.2462 - accuracy: 0.9038 - val_loss: 0.5685 - val_accuracy: 0.7775\n",
      "Epoch 9/15\n",
      "782/782 [==============================] - 76s 97ms/step - loss: 0.1674 - accuracy: 0.9407 - val_loss: 0.6772 - val_accuracy: 0.7839\n",
      "Epoch 10/15\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.1529 - accuracy: 0.9461 - val_loss: 0.6482 - val_accuracy: 0.7737\n",
      "Epoch 11/15\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1304 - accuracy: 0.9558 - val_loss: 0.6728 - val_accuracy: 0.7284\n",
      "Epoch 12/15\n",
      "782/782 [==============================] - 75s 96ms/step - loss: 0.1213 - accuracy: 0.9597 - val_loss: 0.9005 - val_accuracy: 0.7524\n",
      "Epoch 13/15\n",
      "782/782 [==============================] - 76s 97ms/step - loss: 0.3223 - accuracy: 0.8661 - val_loss: 0.6294 - val_accuracy: 0.7005\n",
      "Epoch 14/15\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.3075 - accuracy: 0.8724 - val_loss: 0.8308 - val_accuracy: 0.7173\n",
      "Epoch 15/15\n",
      "782/782 [==============================] - 75s 96ms/step - loss: 0.3796 - accuracy: 0.8338 - val_loss: 0.7164 - val_accuracy: 0.6699\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7164 - accuracy: 0.6699\n",
      "Test score: 0.7163980603218079\n",
      "Test accuracy: 0.6698799729347229\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import keras\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test =  pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(SimpleRNN(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "loss, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', loss)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytf3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
