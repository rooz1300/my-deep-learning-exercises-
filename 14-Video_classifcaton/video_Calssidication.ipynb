{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Classification using CNN-LSTM Model\n",
    "\n",
    "This Jupyter notebook contains code for a video classification model using a combination of Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks. The model is trained on a custom dataset consisting of four classes: 'WalkingWithDog', 'TaiChi', 'Swing', and 'HorseRace'.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports and Setup](#imports-and-setup)\n",
    "2. [Data Preparation](#data-preparation)\n",
    "3. [Model Architecture](#model-architecture)\n",
    "4. [Training the Model](#training-the-model)\n",
    "5. [Evaluation and Saving the Model](#evaluation-and-saving-the-model)\n",
    "\n",
    "## Imports and Setup\n",
    "\n",
    "The code begins by importing the necessary libraries and modules. It also sets up the environment and defines some constants.\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "os.system('cls')\n",
    "\n",
    "seed_constant=27\n",
    "np.random.seed()\n",
    "\n",
    "image_height, image_width= 64,64\n",
    "sequence_length= 20\n",
    "data_dir='UCF50'\n",
    "\n",
    "classes_list=['WalkingWithDog' ,'TaiChi','Swing','HorseRace']\n",
    "```\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "The `frames_extraction` function is defined to extract frames from a video file. The `create_dataset` function then uses this function to create a dataset of frames and corresponding labels. The dataset is split into training and testing sets using the `train_test_split` function from scikit-learn.\n",
    "\n",
    "```python\n",
    "def frames_extraction(video_path):\n",
    "    # ...\n",
    "\n",
    "def create_dataset():\n",
    "    # ...\n",
    "\n",
    "features,labels= create_dataset()\n",
    "one_hot_encoded_label=to_categorical(labels)\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, one_hot_encoded_label, test_size=0.25, random_state=seed_constant)\n",
    "```\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "The `create_LRCN_model` function defines the architecture of the CNN-LSTM model. The model consists of two TimeDistributed Conv2D layers, each followed by BatchNormalization, MaxPool2D, and Dropout layers. The output of these layers is then flattened and fed into an LSTM layer, followed by a Dense layer with softmax activation for classification.\n",
    "\n",
    "```python\n",
    "def create_LRCN_model():\n",
    "    # ...\n",
    "\n",
    "model = create_LRCN_model()\n",
    "```\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "The model is compiled with the Adam optimizer and categorical cross-entropy loss. Early stopping and learning rate reduction are applied to prevent overfitting. The model is then trained on the training set for 100 epochs with a batch size of 20.\n",
    "\n",
    "```python\n",
    "opt=Adam(learning_rate=0.0001)\n",
    "model.compile( optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "model_history = model.fit(train_features, train_labels, epochs=100, batch_size=20,\n",
    "                          shuffle=True, validation_split=0.2, callbacks=[ lr_reduction ,early_stopping])\n",
    "```\n",
    "\n",
    "## Evaluation and Saving the Model\n",
    "\n",
    "The trained model is evaluated on the testing set, and its accuracy and loss are printed. The model is then saved to a file named 'cnn_lstm_classification_me.h5'.\n",
    "\n",
    "```python\n",
    "model_test_eval = model.evaluate(test_features, test_labels)\n",
    "\n",
    "model.save('cnn_lstm_classification_me.h5')\n",
    "```\n",
    "\n",
    "Additionally, the training history is plotted to visualize the training and validation accuracy and loss over epochs.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_history.history['accuracy'] )\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import cv2\n",
    "import random \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau \n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "os.system('cls')\n",
    "\n",
    "seed_constant=27\n",
    "np.random.seed()\n",
    "\n",
    "\n",
    "\n",
    "image_height, image_width= 64,64\n",
    "sequence_length= 20\n",
    "data_dir='UCF50'\n",
    "\n",
    "\n",
    "classes_list=['WalkingWithDog' ,'TaiChi','Swing','HorseRace']\n",
    "\n",
    "\n",
    "def frames_extraction(video_path):\n",
    "  \n",
    "    frames_list=[]\n",
    "    video_reader= cv2.VideoCapture(video_path)\n",
    "    for _ in range(sequence_length):\n",
    "        success,frame= video_reader.read()\n",
    "        if not success: break\n",
    "\n",
    "        resized_frame=cv2.resize(frame,(image_height,image_width))\n",
    "        normalized_frame= resized_frame/255\n",
    "        frames_list.append(normalized_frame)\n",
    "\n",
    "    video_reader.release()    \n",
    "    return frames_list\n",
    "\n",
    "def create_dataset():\n",
    "    features=[]\n",
    "    labels=[]\n",
    "    for class_index , class_name in enumerate(classes_list):\n",
    "        print(f'Extracting data of class : {class_name}')\n",
    "        file_list=os.listdir(os.path.join(data_dir,class_name))\n",
    "        for file in tqdm(file_list):\n",
    "            video_file_path=os.path.join(data_dir,class_name,file)\n",
    "            frames=frames_extraction(video_file_path)\n",
    "            if len(frames) == sequence_length:\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "\n",
    "\n",
    "    features=np.array(features)\n",
    "    labels=np.array(labels)    \n",
    "    return features,labels\n",
    "\n",
    "\n",
    "features,labels= create_dataset()\n",
    "one_hot_encoded_label=to_categorical(labels)\n",
    "        \n",
    "\n",
    "\n",
    " # Split the dataset into train and test sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, one_hot_encoded_label, test_size=0.25, random_state=seed_constant)\n",
    "\n",
    "print(\"Train features shape:\", train_features.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Test features shape:\", test_features.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n",
    "\n",
    "\n",
    "def create_LRCN_model():\n",
    "    l2_reg = 0.01  # You can adjust this value based on your preference\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                              input_shape=(sequence_length, image_height, image_width, 3)))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPool2D(pool_size=(4, 4))))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "\n",
    "    model.add(TimeDistributed(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=l2(l2_reg))))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPool2D(pool_size=(4, 4))))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "\n",
    "    # model.add(TimeDistributed(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=l2(l2_reg))))\n",
    "    # model.add(TimeDistributed(BatchNormalization()))\n",
    "    # model.add(TimeDistributed(MaxPool2D(pool_size=(4, 4))))\n",
    "    # model.add(TimeDistributed(Dropout(0.8)))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(32, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "    # model.add(LSTM(32),k)\n",
    "    model.add(Dense(len(classes_list), activation='softmax', kernel_regularizer=l2(l2_reg)))\n",
    "\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_LRCN_model()\n",
    "\n",
    "opt=Adam(learning_rate=0.0001)\n",
    "model.compile( optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Apply early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "# Store the training history\n",
    "model_history = model.fit(train_features, train_labels, epochs=100, batch_size=20,\n",
    "                          shuffle=True, validation_split=0.2, callbacks=[ lr_reduction ,early_stopping])\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_history.history['accuracy'] )\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "model_test_eval = model.evaluate(test_features, test_labels)\n",
    "\n",
    "model.save('cnn_lstm_classification_me.h5')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
