{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using LSTM Model\n",
    "\n",
    "This Jupyter notebook contains code for a sentiment analysis model using a Long Short-Term Memory (LSTM) network. The model is trained on the IMDB movie reviews dataset.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports and Setup](#imports-and-setup)\n",
    "2. [Data Preparation](#data-preparation)\n",
    "3. [Model Architecture](#model-architecture)\n",
    "4. [Training the Model](#training-the-model)\n",
    "\n",
    "## Imports and Setup\n",
    "\n",
    "The code begins by importing the necessary libraries and modules. It also sets up the environment and defines some constants.\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] =\"3\"\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "batch_size = 32\n",
    "```\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "The training and testing datasets are loaded from the \"aclImdb/train\" and \"aclImdb/test\" directories, respectively. The datasets are batched and shuffled. The `text_vectorization` layer is used to convert the text data to sequences of integers.\n",
    "\n",
    "```python\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8)\n",
    "\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8)\n",
    "```\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "The `model` is defined as a sequential model with an embedding layer, an LSTM layer, and a dense layer with sigmoid activation for binary classification.\n",
    "\n",
    "```python\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.LSTM(32)(embedded)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "```\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "The model is compiled with the RMSprop optimizer and binary cross-entropy loss. The model is then trained on the training set for 10 epochs.\n",
    "\n",
    "```python\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_test_ds, epochs=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'I first saw Martin\\'s Day when I was just 10 years old, at home, on The Movie Channel, and still remember the impact it made on my life. It touched me as no other film had touched me, and I remember balling my eyes out.<br /><br />After the first time I saw it, I couldn\\'t find it anywhere else. I would ask around and no one had ever heard of the film! I guess it was one of those more rare films that not many people knew about, because no one, and I mean no one, knew what I was talking about. I searched and searched throughout the years, checking video stores shelves and scanning cable TV listings, but always came up short. Finally, in 1996 I found out I could special order it, I did, and have probably watched it at least 50 times since--and it still makes me cry, every time.<br /><br />Martin\\'s Day is about Martin Steckert, a man who is in prison (but genuinely a good guy), who yearns to make it back to the special lake where he grew up as boy. This was a special place, where he lived off nature, spent time with his dog, and was left alone to enjoy life. Soon into the movie, he escapes and starts making his way back to the lake.<br /><br />It isn\\'t long before the cops find him, and Steckert grabs a child as a hostage to convince the police to back off. Soon Steckert and his hostage (the 2nd Martin) become best friends, and have many fun adventures together--from robbing a toy truck, to hi-jacking a train, all on the way to this special lake.<br /><br />Throughout the movie, Steckert has great flashbacks of him at the lake as a boy.<br /><br />I won\\'t ruin the ending for you, but I will tell you, this movie is a must see. It is the BEST movie I have EVER seen in my life! I am, without a doubt, the biggest fan of this movie EVER! I managed to find the song that the two Martin\\'s are singing throughout the movie (\"I\\'m going back, to where I come from...). I\\'m even planning a trip to Canada to see the lake and cottage where Martin\\'s Day was filmed. Crazy, I know--but that movie just means so much to me.', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                36992     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,157,025\n",
      "Trainable params: 5,157,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 76s 89ms/step - loss: 0.6932 - accuracy: 0.5032 - val_loss: 0.6943 - val_accuracy: 0.5045\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 61s 77ms/step - loss: 0.6867 - accuracy: 0.5117 - val_loss: 0.6938 - val_accuracy: 0.5102\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 60s 76ms/step - loss: 0.6778 - accuracy: 0.5177 - val_loss: 0.6892 - val_accuracy: 0.5134\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 62s 79ms/step - loss: 0.6309 - accuracy: 0.6378 - val_loss: 0.6940 - val_accuracy: 0.5086\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 61s 77ms/step - loss: 0.5981 - accuracy: 0.6842 - val_loss: 0.6950 - val_accuracy: 0.5057\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 62s 79ms/step - loss: 0.6109 - accuracy: 0.6438 - val_loss: 0.6061 - val_accuracy: 0.7292\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 65s 83ms/step - loss: 0.5555 - accuracy: 0.7429 - val_loss: 0.5954 - val_accuracy: 0.7253\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 63s 80ms/step - loss: 0.5721 - accuracy: 0.7020 - val_loss: 0.7019 - val_accuracy: 0.5314\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 63s 81ms/step - loss: 0.5307 - accuracy: 0.7551 - val_loss: 0.6113 - val_accuracy: 0.7344\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 64s 82ms/step - loss: 0.5246 - accuracy: 0.7566 - val_loss: 0.5533 - val_accuracy: 0.7556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e607786340>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] =\"3\"\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8)\n",
    "\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8)\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.LSTM(32)(embedded)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "              \n",
    "model.summary()\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_test_ds, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytf3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
